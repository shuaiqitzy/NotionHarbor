import asyncio
import json
import os
from playwright.async_api import async_playwright

# ç›®æ ‡æ•°æ®æ–‡ä»¶
OUTPUT_FILE = "my_xhs_data.json"


def load_existing_data():
    """è¯»å–ç°æœ‰çš„ JSON æ–‡ä»¶ï¼Œå¦‚æœä¸å­˜åœ¨è¿”å›ç©ºåˆ—è¡¨"""
    if os.path.exists(OUTPUT_FILE):
        try:
            with open(OUTPUT_FILE, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            print(f"âš ï¸ è¯»å–æ—§æ–‡ä»¶å¤±è´¥ï¼Œå°†åˆ›å»ºæ–°æ–‡ä»¶: {e}")
            return []
    return []


async def extract_single_note_element(el, index):
    """æå–å•ä¸ª DOM å…ƒç´ çš„æ•°æ®"""
    try:
        # 1. æå–é“¾æ¥ (å…³é”®ï¼šç”¨äºè·å– ID)
        link_el = await el.query_selector('a.cover')
        if not link_el:
            link_el = await el.query_selector('a[href^="/explore/"]')

        href = await link_el.get_attribute('href') if link_el else ""
        if not href:
            return None  # æ²¡æœ‰é“¾æ¥é€šå¸¸æ˜¯æ— æ•ˆå…ƒç´ 

        note_id = href.split('/')[-1]
        full_link = f"https://www.xiaohongshu.com{href}"

        # 2. æå–æ ‡é¢˜
        title_el = await el.query_selector('.footer .title span')
        title = await title_el.inner_text() if title_el else "æ— æ ‡é¢˜"

        # 3. æå–å°é¢
        img_el = await el.query_selector('.cover img')
        cover_url = await img_el.get_attribute('src') if img_el else ""

        # 4. æå–ä½œè€…
        author_el = await el.query_selector('.author-wrapper .name')
        author = await author_el.inner_text() if author_el else "æœªçŸ¥ä½œè€…"

        # 5. æå–å¤´åƒ
        avatar_el = await el.query_selector('.author-wrapper img')
        avatar = await avatar_el.get_attribute('src') if avatar_el else ""

        # 6. æå–ç‚¹èµ
        like_el = await el.query_selector('.like-wrapper .count')
        likes = await like_el.inner_text() if like_el else "0"

        is_video = await el.query_selector('.play-icon') is not None

        return {
            "id": note_id,
            "title": title,
            "cover": cover_url,
            "author": author,
            "authorAvatar": avatar,
            "type": "video" if is_video else "normal",
            "likes": likes,
            "collects": 0,
            "link": full_link,
            "tags": []
        }
    except Exception as e:
        # æŸäº›ç‰¹æ®Šå¹¿å‘Šä½æˆ–æ— æ•ˆå…ƒç´ å¯èƒ½ä¼šæŠ¥é”™ï¼Œå¿½ç•¥å³å¯
        return None


async def scrape_album_incrementally(page, album_name, existing_album_notes):
    """
    è¾¹æ»šåŠ¨è¾¹æŠ“å–ï¼Œå¹¶ä¸æ—§æ•°æ®åˆå¹¶
    """
    # å°†æ—§ç¬”è®°è½¬ä¸ºå­—å…¸ï¼ŒKey ä¸º IDï¼Œæ–¹ä¾¿å¿«é€ŸæŸ¥æ‰¾å’Œæ›´æ–°
    # ç»“æ„: { "note_id_1": {data...}, "note_id_2": {data...} }
    notes_map = {note['id']: note for note in existing_album_notes}

    print(f">>> å¼€å§‹æŠ“å–ä¸“è¾‘ '{album_name}'...")
    print(f"    å½“å‰å·²æœ‰å­˜æ¡£ç¬”è®°: {len(notes_map)} ç¯‡")

    # æ»šåŠ¨æ§åˆ¶å˜é‡
    no_change_count = 0
    max_no_change = 5  # è¿ç»­5æ¬¡é«˜åº¦ä¸å˜åˆ™è®¤ä¸ºåˆ°åº•
    previous_height = 0

    scraped_count_session = 0

    while True:
        # 1. --- æŠ“å–å½“å‰è§†å£å†…çš„æ‰€æœ‰ç¬”è®° ---
        # æ³¨æ„ï¼šè¿™é‡Œä¼šåŒ…å«ä¹‹å‰æŠ“è¿‡çš„ï¼Œä¹Ÿä¼šåŒ…å«æ–°åŠ è½½çš„
        elements = await page.query_selector_all('section.note-item')

        for idx, el in enumerate(elements):
            note_data = await extract_single_note_element(el, idx)
            if note_data:
                # ã€å¢é‡æ›´æ–°é€»è¾‘ã€‘
                # æ— è®º ID æ˜¯å¦å­˜åœ¨ï¼Œéƒ½ç”¨æ–°æŠ“å–çš„æ•°æ®è¦†ç›–ï¼ˆä¿è¯ç‚¹èµæ•°ã€æ ‡é¢˜æ˜¯æœ€æ–°çš„ï¼‰
                # æˆ–è€…å¦‚æœä½ æƒ³ä¿ç•™æ—§æ•°æ®çš„æŸäº›å­—æ®µï¼Œå¯ä»¥åœ¨è¿™é‡ŒåŠ åˆ¤æ–­
                if note_data['id'] not in notes_map:
                    scraped_count_session += 1

                notes_map[note_data['id']] = note_data

        # 2. --- æ»šåŠ¨é¡µé¢ ---
        # æ¯æ¬¡å‘ä¸‹æ»šåŠ¨çº¦ 800px (æ¨¡æ‹Ÿç”¨æˆ·è¡Œä¸º)
        await page.evaluate("window.scrollBy(0, 800)")
        await page.wait_for_timeout(1000)  # ç­‰å¾…åŠ è½½

        # 3. --- æ£€æŸ¥æ˜¯å¦åˆ°åº• ---
        current_height = await page.evaluate("document.body.scrollHeight")
        current_scroll_y = await page.evaluate("window.scrollY")
        viewport_height = await page.evaluate("window.innerHeight")

        # å¦‚æœå½“å‰æ»šåŠ¨ä½ç½® + è§†å£é«˜åº¦ æ¥è¿‘ æ€»é«˜åº¦ï¼Œæˆ–è€…é«˜åº¦ä¸å†å˜åŒ–
        if current_height == previous_height:
            no_change_count += 1
            print(f"    é¡µé¢é«˜åº¦æœªå˜åŒ– ({no_change_count}/{max_no_change})...")
        else:
            no_change_count = 0
            previous_height = current_height
            print(f"    æ­£åœ¨åŠ è½½... (åº“ä¸­å½“å‰å…± {len(notes_map)} ç¯‡)")

        if no_change_count >= max_no_change:
            print(">>> åˆ¤å®šå·²åˆ°è¾¾åº•éƒ¨ã€‚")
            break

    print(f"âœ… ä¸“è¾‘ '{album_name}' å¤„ç†å®Œæ¯•ã€‚æœ¬æ¬¡æ–°å¢/æ›´æ–°: {scraped_count_session} ç¯‡ (æ€»è®¡: {len(notes_map)} ç¯‡)")

    # å°†å­—å…¸è½¬å›åˆ—è¡¨è¿”å›
    return list(notes_map.values())


async def main():
    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=False)
        context = await browser.new_context(
            viewport={'width': 1280, 'height': 800},
            user_agent="Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
        )
        page = await context.new_page()

        # è¯»å–ç°æœ‰çš„ JSON æ•°æ®
        all_data = load_existing_data()

        print("æ­£åœ¨æ‰“å¼€å°çº¢ä¹¦...")
        await page.goto("https://www.xiaohongshu.com/explore")

        print("\n" + "=" * 50)
        print("ã€æ“ä½œæŒ‡å¼•ã€‘")
        print("1. è¯·æ‰«ç ç™»å½•ã€‚")
        print("2. è¿›å…¥ä¸ªäººä¸­å¿ƒ -> ç‚¹å‡»ã€æˆ‘çš„æ”¶è—ã€‘ã€‚")
        print("3. ç‚¹å‡»è¿›å…¥å…·ä½“çš„ä¸“è¾‘ã€‚")
        print("=" * 50 + "\n")

        while True:
            album_name = input("\nè¯·è¾“å…¥å½“å‰ä¸“è¾‘åç§° (è¾“å…¥ q ä¿å­˜å¹¶é€€å‡º): ").strip()
            if album_name.lower() == 'q':
                break

            # æŸ¥æ‰¾è¯¥ä¸“è¾‘ä¹‹å‰çš„æ—§æ•°æ®
            existing_album_index = -1
            existing_album_notes = []

            for idx, album in enumerate(all_data):
                if album['name'] == album_name:
                    existing_album_index = idx
                    existing_album_notes = album['notes']
                    break

            # æ‰§è¡Œå¢é‡æŠ“å–
            updated_notes = await scrape_album_incrementally(page, album_name, existing_album_notes)

            # æ›´æ–°æ€»æ•°æ®ç»“æ„
            new_album_data = {
                "name": album_name,
                "notes": updated_notes
            }

            if existing_album_index != -1:
                all_data[existing_album_index] = new_album_data
            else:
                all_data.append(new_album_data)

            # ä¸ºäº†å®‰å…¨èµ·è§ï¼Œæ¯çˆ¬å®Œä¸€ä¸ªä¸“è¾‘å°±ä¿å­˜ä¸€æ¬¡æ–‡ä»¶
            # è¿™æ ·å¦‚æœä¸­é€”æŠ¥é”™ï¼Œå‰é¢çš„æ•°æ®ä¸ä¼šä¸¢
            with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:
                json.dump(all_data, f, ensure_ascii=False, indent=2)

            print(f"ğŸ’¾ æ•°æ®å·²è‡ªåŠ¨ä¿å­˜è‡³ {OUTPUT_FILE}")

            print(">>> è¯·åˆ‡æ¢åˆ°ä¸‹ä¸€ä¸ªä¸“è¾‘ï¼Œç„¶åç»§ç»­...")

        await browser.close()


if __name__ == "__main__":
    asyncio.run(main())