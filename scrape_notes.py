"""
å°çº¢ä¹¦æ”¶è—å¤¹æœ¬åœ°åŒ– - ç¬”è®°çˆ¬å–è„šæœ¬ v2.0
è¯»å– my_xhs_data.json ä¸­çš„ç¬”è®°åˆ—è¡¨ï¼Œä½¿ç”¨ MediaCrawler çˆ¬å–è¯¦æƒ…å¹¶ä¿å­˜åˆ°æœ¬åœ°

ç‰¹æ€§ï¼š
- æ™ºèƒ½æ£€æµ‹ï¼šé€šè¿‡ç¬”è®° ID æ£€æµ‹æ˜¯å¦å·²ä¸‹è½½ï¼Œæ”¯æŒå¢é‡æ›´æ–°
- æ–­ç‚¹ç»­çˆ¬ï¼šè·³è¿‡å·²ä¸‹è½½çš„ç¬”è®°ï¼Œåªçˆ¬å–æ–°å¢å†…å®¹
- åª’ä½“ä¸‹è½½ï¼šè‡ªåŠ¨ä¸‹è½½å›¾ç‰‡å’Œè§†é¢‘åˆ°æœ¬åœ°
"""

import asyncio
import json
import os
import re
import sys
from pathlib import Path
from typing import Dict, List, Optional, Set
from urllib.parse import urlparse, parse_qs

import aiofiles
import aiohttp

# æ·»åŠ  MediaCrawler åˆ°è·¯å¾„
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'MediaCrawler'))

from playwright.async_api import async_playwright, BrowserContext, Page

# MediaCrawler å¯¼å…¥
from MediaCrawler.media_platform.xhs.client import XiaoHongShuClient
from MediaCrawler.media_platform.xhs.login import XiaoHongShuLogin
from MediaCrawler.media_platform.xhs.help import parse_note_info_from_note_url
from MediaCrawler.tools import utils
from MediaCrawler.tools.cdp_browser import CDPBrowserManager

# ================= é…ç½® =================
SOURCE_FILE = "my_xhs_data.json"           # æ”¶è—å¤¹æ•°æ®æ–‡ä»¶
DATA_DIR = "data_storage"                    # æœ¬åœ°å­˜å‚¨ç›®å½•
COOKIE_FILE = "cookie.txt"                   # Cookie æ–‡ä»¶ï¼ˆå¯é€‰ï¼‰

# çˆ¬å–é…ç½®
ENABLE_CDP_MODE = True                       # æ˜¯å¦ä½¿ç”¨ CDP æ¨¡å¼ï¼ˆæ¨èï¼‰
HEADLESS = False                             # æ˜¯å¦æ— å¤´æ¨¡å¼ï¼ˆå»ºè®® False æ–¹ä¾¿ç™»å½•ï¼‰
CRAWLER_SLEEP_SEC = 2                        # çˆ¬å–é—´éš”ï¼ˆç§’ï¼‰
MAX_CONCURRENCY = 2                          # å¹¶å‘æ•°
DOWNLOAD_MEDIA = True                        # æ˜¯å¦ä¸‹è½½å›¾ç‰‡å’Œè§†é¢‘

# ========================================


def sanitize_filename(name: str) -> str:
    """æ¸…æ´—æ–‡ä»¶åï¼Œç§»é™¤éæ³•å­—ç¬¦"""
    if not name:
        return "untitled"
    # ç§»é™¤ Windows æ–‡ä»¶åéæ³•å­—ç¬¦
    name = re.sub(r'[<>:"/\\|?*\n\r\t]', '_', name)
    # ç§»é™¤å‰åç©ºç™½å’Œç‚¹
    name = name.strip(' .')
    # é™åˆ¶é•¿åº¦
    return name[:80] if name else "untitled"


def scan_downloaded_notes(album_dir: str) -> Set[str]:
    """æ‰«æå·²ä¸‹è½½çš„ç¬”è®° ID é›†åˆ"""
    downloaded_ids = set()
    
    if not os.path.exists(album_dir):
        return downloaded_ids
    
    for folder_name in os.listdir(album_dir):
        folder_path = os.path.join(album_dir, folder_name)
        metadata_path = os.path.join(folder_path, "metadata.json")
        
        # åªæœ‰å­˜åœ¨ metadata.json æ‰ç®—å·²ä¸‹è½½
        if os.path.isdir(folder_path) and os.path.exists(metadata_path):
            # ä»æ–‡ä»¶å¤¹åæå–ç¬”è®° IDï¼ˆæ ¼å¼ï¼štitle_noteIdï¼‰
            parts = folder_name.rsplit('_', 1)
            if len(parts) == 2:
                downloaded_ids.add(parts[1])
    
    return downloaded_ids


def find_existing_note_folder(album_dir: str, note_id: str) -> Optional[str]:
    """æŸ¥æ‰¾å·²å­˜åœ¨çš„ç¬”è®°æ–‡ä»¶å¤¹ï¼ˆé€šè¿‡ç¬”è®° IDï¼‰"""
    if not os.path.exists(album_dir):
        return None
    
    for folder_name in os.listdir(album_dir):
        if folder_name.endswith(f"_{note_id}"):
            folder_path = os.path.join(album_dir, folder_name)
            metadata_path = os.path.join(folder_path, "metadata.json")
            if os.path.exists(metadata_path):
                return folder_path
    
    return None


def parse_note_id_from_url(note_url: str) -> tuple:
    """ä» URL ä¸­è§£æç¬”è®° ID å’Œ token ä¿¡æ¯"""
    # URL æ ¼å¼: https://www.xiaohongshu.com/board/xxx/note_id?xsec_token=xxx&xsec_source=xxx
    # æˆ–è€… id ç›´æ¥åŒ…å« ?xsec_token=xxx
    
    if '?' in note_url:
        base_part, query_part = note_url.split('?', 1)
    else:
        base_part = note_url
        query_part = ""
    
    # æå–ç¬”è®° ID
    note_id = base_part.split('/')[-1] if '/' in base_part else base_part
    
    # è§£æ token å‚æ•°
    params = {}
    if query_part:
        for param in query_part.split('&'):
            if '=' in param:
                key, value = param.split('=', 1)
                params[key] = value
    
    xsec_token = params.get('xsec_token', '')
    xsec_source = params.get('xsec_source', 'pc_feed')
    
    return note_id, xsec_token, xsec_source


def parse_note_id_from_item(note_item: dict) -> tuple:
    """ä»ç¬”è®°é¡¹ä¸­è§£æ ID å’Œ token"""
    raw_id = note_item.get('id', '')
    
    # ID å¯èƒ½åŒ…å« ?xsec_token=xxx æ ¼å¼
    if '?' in raw_id:
        note_id, query_part = raw_id.split('?', 1)
        params = {}
        for param in query_part.split('&'):
            if '=' in param:
                key, value = param.split('=', 1)
                params[key] = value
        xsec_token = params.get('xsec_token', '')
        xsec_source = params.get('xsec_source', 'pc_feed')
    else:
        note_id = raw_id
        # å°è¯•ä» link ä¸­è·å– token
        link = note_item.get('link', '')
        if '?' in link:
            _, query_part = link.split('?', 1)
            params = {}
            for param in query_part.split('&'):
                if '=' in param:
                    key, value = param.split('=', 1)
                    params[key] = value
            xsec_token = params.get('xsec_token', '')
            xsec_source = params.get('xsec_source', 'pc_feed')
        else:
            xsec_token = ''
            xsec_source = 'pc_feed'
    
    return note_id, xsec_token, xsec_source


class FavoriteCrawler:
    """æ”¶è—å¤¹çˆ¬è™«"""
    
    def __init__(self):
        self.browser_context: Optional[BrowserContext] = None
        self.context_page: Optional[Page] = None
        self.xhs_client: Optional[XiaoHongShuClient] = None
        self.cdp_manager: Optional[CDPBrowserManager] = None
        self.user_agent = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
        
    async def initialize(self):
        """åˆå§‹åŒ–æµè§ˆå™¨å’Œå®¢æˆ·ç«¯"""
        print("ğŸš€ æ­£åœ¨åˆå§‹åŒ–æµè§ˆå™¨...")
        
        async with async_playwright() as playwright:
            await self._launch_browser(playwright)
            await self._create_client()
            
            # æ‰§è¡Œçˆ¬å–
            await self._run_crawl()
            
            # æ¸…ç†
            await self._cleanup()
    
    async def _launch_browser(self, playwright):
        """å¯åŠ¨æµè§ˆå™¨"""
        if ENABLE_CDP_MODE:
            print("ğŸ“Œ ä½¿ç”¨ CDP æ¨¡å¼å¯åŠ¨æµè§ˆå™¨...")
            try:
                self.cdp_manager = CDPBrowserManager()
                self.browser_context = await self.cdp_manager.launch_and_connect(
                    playwright=playwright,
                    playwright_proxy=None,
                    user_agent=self.user_agent,
                    headless=HEADLESS,
                )
            except Exception as e:
                print(f"âš ï¸ CDP æ¨¡å¼å¯åŠ¨å¤±è´¥ï¼Œä½¿ç”¨æ ‡å‡†æ¨¡å¼: {e}")
                self.cdp_manager = None
                await self._launch_standard_browser(playwright)
        else:
            await self._launch_standard_browser(playwright)
        
        self.context_page = await self.browser_context.new_page()
        await self.context_page.goto("https://www.xiaohongshu.com")
        print("âœ… æµè§ˆå™¨å¯åŠ¨æˆåŠŸ")
    
    async def _launch_standard_browser(self, playwright):
        """æ ‡å‡†æ¨¡å¼å¯åŠ¨æµè§ˆå™¨"""
        user_data_dir = os.path.join(os.getcwd(), "browser_data", "xhs_user_data_dir")
        self.browser_context = await playwright.chromium.launch_persistent_context(
            user_data_dir=user_data_dir,
            accept_downloads=True,
            headless=HEADLESS,
            viewport={"width": 1920, "height": 1080},
            user_agent=self.user_agent,
        )
        # åŠ è½½åæ£€æµ‹è„šæœ¬
        stealth_js = os.path.join(os.path.dirname(__file__), 'MediaCrawler', 'libs', 'stealth.min.js')
        if os.path.exists(stealth_js):
            await self.browser_context.add_init_script(path=stealth_js)
    
    async def _create_client(self):
        """åˆ›å»º XHS å®¢æˆ·ç«¯"""
        print("ğŸ”§ æ­£åœ¨åˆ›å»ºå®¢æˆ·ç«¯...")
        
        cookie_str, cookie_dict = utils.convert_cookies(await self.browser_context.cookies())
        
        self.xhs_client = XiaoHongShuClient(
            proxy=None,
            headers={
                "accept": "application/json, text/plain, */*",
                "accept-language": "zh-CN,zh;q=0.9",
                "content-type": "application/json;charset=UTF-8",
                "origin": "https://www.xiaohongshu.com",
                "referer": "https://www.xiaohongshu.com/",
                "user-agent": self.user_agent,
                "Cookie": cookie_str,
            },
            playwright_page=self.context_page,
            cookie_dict=cookie_dict,
        )
        
        # æ£€æŸ¥ç™»å½•çŠ¶æ€
        if not await self.xhs_client.pong():
            print("âš ï¸ æœªç™»å½•ï¼Œè¯·æ‰«ç ç™»å½•...")
            login_obj = XiaoHongShuLogin(
                login_type="qrcode",
                login_phone="",
                browser_context=self.browser_context,
                context_page=self.context_page,
                cookie_str="",
            )
            await login_obj.begin()
            await self.xhs_client.update_cookies(browser_context=self.browser_context)
        
        print("âœ… å®¢æˆ·ç«¯åˆ›å»ºæˆåŠŸï¼Œç™»å½•çŠ¶æ€æ­£å¸¸")
    
    async def _run_crawl(self):
        """æ‰§è¡Œçˆ¬å–ä»»åŠ¡"""
        # è¯»å–æ”¶è—å¤¹æ•°æ®
        if not os.path.exists(SOURCE_FILE):
            print(f"âŒ æ‰¾ä¸åˆ°æ•°æ®æ–‡ä»¶: {SOURCE_FILE}")
            return
        
        with open(SOURCE_FILE, 'r', encoding='utf-8') as f:
            albums = json.load(f)
        
        print(f"\nğŸ“š å…±æ‰¾åˆ° {len(albums)} ä¸ªä¸“è¾‘")
        
        # é¢„æ‰«æï¼šç»Ÿè®¡æ‰€æœ‰ç¬”è®°å’Œå·²ä¸‹è½½ç¬”è®°
        total_notes = 0
        total_downloaded = 0
        total_new = 0
        
        album_stats = []
        for album in albums:
            album_name = album.get('name', 'æœªå‘½åä¸“è¾‘')
            notes = album.get('notes', [])
            safe_album_name = sanitize_filename(album_name)
            album_dir = os.path.join(DATA_DIR, safe_album_name)
            
            # æ‰«æè¯¥ä¸“è¾‘å·²ä¸‹è½½çš„ç¬”è®° ID
            downloaded_ids = scan_downloaded_notes(album_dir)
            
            # ç»Ÿè®¡æ–°å¢ç¬”è®°
            new_notes = []
            for note in notes:
                note_id, _, _ = parse_note_id_from_item(note)
                if note_id not in downloaded_ids:
                    new_notes.append(note)
            
            album_stats.append({
                'name': album_name,
                'notes': notes,
                'new_notes': new_notes,
                'downloaded_ids': downloaded_ids,
                'album_dir': album_dir,
            })
            
            total_notes += len(notes)
            total_downloaded += len(downloaded_ids)
            total_new += len(new_notes)
        
        print(f"\nğŸ“Š ç¬”è®°ç»Ÿè®¡:")
        print(f"   ğŸ“ æ€»ç¬”è®°æ•°: {total_notes}")
        print(f"   âœ… å·²ä¸‹è½½: {total_downloaded}")
        print(f"   ğŸ†• å¾…ä¸‹è½½: {total_new}")
        
        if total_new == 0:
            print(f"\nâœ¨ æ‰€æœ‰ç¬”è®°éƒ½å·²ä¸‹è½½ï¼Œæ— éœ€æ›´æ–°ï¼")
            return
        
        print(f"\nğŸš€ å¼€å§‹çˆ¬å– {total_new} æ¡æ–°ç¬”è®°...\n")
        
        # ç»Ÿè®¡
        downloaded = 0
        skipped = 0
        failed = 0
        
        for album_info in album_stats:
            album_name = album_info['name']
            new_notes = album_info['new_notes']
            album_dir = album_info['album_dir']
            total_album_notes = len(album_info['notes'])
            
            if not new_notes:
                print(f"ğŸ“ {album_name}: æ— æ–°å¢ç¬”è®°ï¼Œè·³è¿‡")
                continue
            
            print(f"\n{'='*50}")
            print(f"ğŸ“ ä¸“è¾‘: {album_name}")
            print(f"   æ€»æ•°: {total_album_notes} | å·²ä¸‹è½½: {len(album_info['downloaded_ids'])} | æ–°å¢: {len(new_notes)}")
            print('='*50)
            
            os.makedirs(album_dir, exist_ok=True)
            
            for i, note_item in enumerate(new_notes, 1):
                note_id, xsec_token, xsec_source = parse_note_id_from_item(note_item)
                note_title = note_item.get('title', '')
                
                # å†æ¬¡æ£€æŸ¥ï¼ˆé˜²æ­¢å¹¶å‘é—®é¢˜ï¼‰
                existing_folder = find_existing_note_folder(album_dir, note_id)
                if existing_folder:
                    print(f"  â­ï¸ [{i}/{len(new_notes)}] å·²å­˜åœ¨: {note_title[:30]}...")
                    skipped += 1
                    continue
                
                print(f"  ğŸ†• [{i}/{len(new_notes)}] æ­£åœ¨çˆ¬å–: {note_title[:40]}...")
                
                try:
                    # è·å–ç¬”è®°è¯¦æƒ…
                    note_detail = await self._get_note_detail(note_id, xsec_token, xsec_source)
                    
                    if note_detail:
                        # æ„å»ºä¿å­˜è·¯å¾„
                        safe_title = sanitize_filename(note_title)
                        note_folder = f"{safe_title}_{note_id}"
                        note_dir = os.path.join(album_dir, note_folder)
                        
                        # ä¿å­˜æ•°æ®
                        await self._save_note(
                            note_dir=note_dir,
                            note_detail=note_detail,
                            album_name=album_name,
                            original_item=note_item
                        )
                        downloaded += 1
                        print(f"      âœ… ä¿å­˜æˆåŠŸ")
                    else:
                        failed += 1
                        print(f"      âŒ è·å–è¯¦æƒ…å¤±è´¥")
                    
                    # çˆ¬å–é—´éš”
                    await asyncio.sleep(CRAWLER_SLEEP_SEC)
                    
                except Exception as e:
                    failed += 1
                    print(f"      âŒ é”™è¯¯: {e}")
        
        # æ‰“å°ç»Ÿè®¡
        print(f"\n{'='*50}")
        print(f"ğŸ“Š æœ¬æ¬¡çˆ¬å–ç»Ÿè®¡:")
        print(f"   âœ… æ–°ä¸‹è½½: {downloaded}")
        print(f"   â­ï¸ è·³è¿‡: {skipped}")
        print(f"   âŒ å¤±è´¥: {failed}")
        print(f"   ğŸ“ å¤„ç†: {downloaded + skipped + failed}/{total_new}")
        print('='*50)
        print(f"\nğŸ“¦ æœ¬åœ°æ€»è®¡: {total_downloaded + downloaded} æ¡ç¬”è®°")
    
    async def _get_note_detail(self, note_id: str, xsec_token: str, xsec_source: str) -> Optional[Dict]:
        """è·å–ç¬”è®°è¯¦æƒ…"""
        try:
            # å°è¯• API æ–¹å¼
            note_detail = await self.xhs_client.get_note_by_id(note_id, xsec_source, xsec_token)
            
            if not note_detail:
                # å°è¯• HTML æ–¹å¼
                note_detail = await self.xhs_client.get_note_by_id_from_html(
                    note_id, xsec_source, xsec_token, enable_cookie=True
                )
            
            if note_detail:
                note_detail.update({
                    "xsec_token": xsec_token,
                    "xsec_source": xsec_source
                })
            
            return note_detail
            
        except Exception as e:
            print(f"      âš ï¸ è·å–è¯¦æƒ…å¼‚å¸¸: {e}")
            return None
    
    async def _save_note(self, note_dir: str, note_detail: Dict, album_name: str, original_item: Dict):
        """ä¿å­˜ç¬”è®°åˆ°æœ¬åœ°"""
        os.makedirs(note_dir, exist_ok=True)
        
        # å‡†å¤‡ metadata
        metadata = {
            "note_id": note_detail.get("note_id", ""),
            "title": note_detail.get("title", ""),
            "desc": note_detail.get("desc", ""),
            "type": note_detail.get("type", "normal"),
            "user": {
                "user_id": note_detail.get("user_id", ""),
                "nickname": note_detail.get("nickname", ""),
                "avatar": note_detail.get("avatar", original_item.get("authorAvatar", "")),
            },
            "interact_info": {
                "liked_count": note_detail.get("liked_count", 0),
                "collected_count": note_detail.get("collected_count", 0),
                "comment_count": note_detail.get("comment_count", 0),
                "share_count": note_detail.get("share_count", 0),
            },
            "tag_list": note_detail.get("tag_list", []),
            "image_list": note_detail.get("image_list", []),
            "video_url": note_detail.get("video_url", ""),
            "time": note_detail.get("time", ""),
            "last_update_time": note_detail.get("last_update_time", ""),
            "album": album_name,
            "note_url": f"https://www.xiaohongshu.com/explore/{note_detail.get('note_id', '')}",
            "xsec_token": note_detail.get("xsec_token", ""),
        }
        
        # ä¿å­˜ metadata.json
        metadata_path = os.path.join(note_dir, "metadata.json")
        async with aiofiles.open(metadata_path, 'w', encoding='utf-8') as f:
            await f.write(json.dumps(metadata, ensure_ascii=False, indent=2))
        
        # ä¸‹è½½åª’ä½“æ–‡ä»¶
        if DOWNLOAD_MEDIA:
            await self._download_media(note_dir, note_detail)
    
    async def _download_media(self, note_dir: str, note_detail: Dict):
        """ä¸‹è½½åª’ä½“æ–‡ä»¶ï¼ˆå›¾ç‰‡å’Œè§†é¢‘ï¼‰"""
        # ä¸‹è½½å›¾ç‰‡
        image_list = note_detail.get("image_list", [])
        for idx, img in enumerate(image_list):
            url = img.get("url_default") or img.get("url") or ""
            if not url:
                continue
            
            try:
                content = await self.xhs_client.get_note_media(url)
                if content:
                    img_path = os.path.join(note_dir, f"image_{idx}.jpg")
                    async with aiofiles.open(img_path, 'wb') as f:
                        await f.write(content)
                    await asyncio.sleep(0.5)  # é¿å…è¯·æ±‚è¿‡å¿«
            except Exception as e:
                print(f"      âš ï¸ å›¾ç‰‡ä¸‹è½½å¤±è´¥: {e}")
        
        # ä¸‹è½½è§†é¢‘
        video_url = note_detail.get("video_url", "")
        if not video_url:
            # å°è¯•ä»å…¶ä»–å­—æ®µè·å–è§†é¢‘ URL
            video_info = note_detail.get("video", {})
            if isinstance(video_info, dict):
                media = video_info.get("media", {})
                stream = media.get("stream", {})
                for quality in ["h266", "h265", "h264", "av1"]:
                    streams = stream.get(quality, [])
                    if streams:
                        video_url = streams[0].get("master_url", "")
                        if video_url:
                            break
        
        if video_url:
            try:
                content = await self.xhs_client.get_note_media(video_url)
                if content:
                    video_path = os.path.join(note_dir, "video.mp4")
                    async with aiofiles.open(video_path, 'wb') as f:
                        await f.write(content)
            except Exception as e:
                print(f"      âš ï¸ è§†é¢‘ä¸‹è½½å¤±è´¥: {e}")
    
    async def _cleanup(self):
        """æ¸…ç†èµ„æº"""
        print("\nğŸ§¹ æ­£åœ¨æ¸…ç†èµ„æº...")
        if self.cdp_manager:
            await self.cdp_manager.cleanup()
        elif self.browser_context:
            await self.browser_context.close()
        print("âœ… æ¸…ç†å®Œæˆ")


async def main():
    """ä¸»å‡½æ•°"""
    print("""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘         å°çº¢ä¹¦æ”¶è—å¤¹æœ¬åœ°åŒ– - ç¬”è®°çˆ¬å–å·¥å…· v2.0            â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘  åŠŸèƒ½ï¼šè¯»å–æ”¶è—å¤¹æ•°æ®ï¼Œçˆ¬å–ç¬”è®°è¯¦æƒ…å’Œåª’ä½“æ–‡ä»¶åˆ°æœ¬åœ°        â•‘
â•‘  ç‰¹æ€§ï¼šæ™ºèƒ½æ£€æµ‹å·²ä¸‹è½½ç¬”è®°ï¼Œæ”¯æŒå¢é‡æ›´æ–°                    â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """)
    
    # æ£€æŸ¥æ•°æ®æ–‡ä»¶
    if not os.path.exists(SOURCE_FILE):
        print(f"âŒ é”™è¯¯ï¼šæ‰¾ä¸åˆ°æ•°æ®æ–‡ä»¶ {SOURCE_FILE}")
        print("è¯·å…ˆè¿è¡Œæ”¶è—å¤¹è·å–è„šæœ¬ç”Ÿæˆæ•°æ®æ–‡ä»¶")
        return
    
    # åˆ›å»ºå­˜å‚¨ç›®å½•
    os.makedirs(DATA_DIR, exist_ok=True)
    
    crawler = FavoriteCrawler()
    
    try:
        # ä½¿ç”¨ async_playwright çš„æ­£ç¡®æ–¹å¼
        async with async_playwright() as playwright:
            # å¯åŠ¨æµè§ˆå™¨
            if ENABLE_CDP_MODE:
                print("ğŸ“Œ ä½¿ç”¨ CDP æ¨¡å¼å¯åŠ¨æµè§ˆå™¨...")
                try:
                    crawler.cdp_manager = CDPBrowserManager()
                    crawler.browser_context = await crawler.cdp_manager.launch_and_connect(
                        playwright=playwright,
                        playwright_proxy=None,
                        user_agent=crawler.user_agent,
                        headless=HEADLESS,
                    )
                except Exception as e:
                    print(f"âš ï¸ CDP æ¨¡å¼å¯åŠ¨å¤±è´¥ï¼Œä½¿ç”¨æ ‡å‡†æ¨¡å¼: {e}")
                    crawler.cdp_manager = None
                    user_data_dir = os.path.join(os.getcwd(), "browser_data", "xhs_user_data_dir")
                    crawler.browser_context = await playwright.chromium.launch_persistent_context(
                        user_data_dir=user_data_dir,
                        accept_downloads=True,
                        headless=HEADLESS,
                        viewport={"width": 1920, "height": 1080},
                        user_agent=crawler.user_agent,
                    )
                    stealth_js = os.path.join(os.path.dirname(__file__), 'MediaCrawler', 'libs', 'stealth.min.js')
                    if os.path.exists(stealth_js):
                        await crawler.browser_context.add_init_script(path=stealth_js)
            else:
                user_data_dir = os.path.join(os.getcwd(), "browser_data", "xhs_user_data_dir")
                crawler.browser_context = await playwright.chromium.launch_persistent_context(
                    user_data_dir=user_data_dir,
                    accept_downloads=True,
                    headless=HEADLESS,
                    viewport={"width": 1920, "height": 1080},
                    user_agent=crawler.user_agent,
                )
                stealth_js = os.path.join(os.path.dirname(__file__), 'MediaCrawler', 'libs', 'stealth.min.js')
                if os.path.exists(stealth_js):
                    await crawler.browser_context.add_init_script(path=stealth_js)
            
            crawler.context_page = await crawler.browser_context.new_page()
            await crawler.context_page.goto("https://www.xiaohongshu.com")
            print("âœ… æµè§ˆå™¨å¯åŠ¨æˆåŠŸ")
            
            # åˆ›å»ºå®¢æˆ·ç«¯
            print("ğŸ”§ æ­£åœ¨åˆ›å»ºå®¢æˆ·ç«¯...")
            cookie_str, cookie_dict = utils.convert_cookies(await crawler.browser_context.cookies())
            
            crawler.xhs_client = XiaoHongShuClient(
                proxy=None,
                headers={
                    "accept": "application/json, text/plain, */*",
                    "accept-language": "zh-CN,zh;q=0.9",
                    "content-type": "application/json;charset=UTF-8",
                    "origin": "https://www.xiaohongshu.com",
                    "referer": "https://www.xiaohongshu.com/",
                    "user-agent": crawler.user_agent,
                    "Cookie": cookie_str,
                },
                playwright_page=crawler.context_page,
                cookie_dict=cookie_dict,
            )
            
            # æ£€æŸ¥ç™»å½•çŠ¶æ€
            if not await crawler.xhs_client.pong():
                print("âš ï¸ æœªç™»å½•ï¼Œè¯·æ‰«ç ç™»å½•...")
                login_obj = XiaoHongShuLogin(
                    login_type="qrcode",
                    login_phone="",
                    browser_context=crawler.browser_context,
                    context_page=crawler.context_page,
                    cookie_str="",
                )
                await login_obj.begin()
                await crawler.xhs_client.update_cookies(browser_context=crawler.browser_context)
            
            print("âœ… å®¢æˆ·ç«¯åˆ›å»ºæˆåŠŸï¼Œç™»å½•çŠ¶æ€æ­£å¸¸")
            
            # æ‰§è¡Œçˆ¬å–
            await crawler._run_crawl()
            
            # æ¸…ç†
            print("\nğŸ§¹ æ­£åœ¨æ¸…ç†èµ„æº...")
            if crawler.cdp_manager:
                await crawler.cdp_manager.cleanup()
            elif crawler.browser_context:
                await crawler.browser_context.close()
            print("âœ… æ¸…ç†å®Œæˆ")
            
    except KeyboardInterrupt:
        print("\n\nâš ï¸ ç”¨æˆ·ä¸­æ–­ï¼Œæ­£åœ¨é€€å‡º...")
    except Exception as e:
        print(f"\nâŒ å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    asyncio.run(main())

